{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9893720,"sourceType":"datasetVersion","datasetId":6076642},{"sourceId":9893901,"sourceType":"datasetVersion","datasetId":6076769},{"sourceId":9897713,"sourceType":"datasetVersion","datasetId":6079687}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install transformers\n!pip install transformers tensorflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:43:02.035986Z","iopub.execute_input":"2024-11-14T09:43:02.036424Z","iopub.status.idle":"2024-11-14T09:43:14.886552Z","shell.execute_reply.started":"2024-11-14T09:43:02.036386Z","shell.execute_reply":"2024-11-14T09:43:14.885374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install stanza","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:43:14.888514Z","iopub.execute_input":"2024-11-14T09:43:14.888827Z","iopub.status.idle":"2024-11-14T09:43:27.588955Z","shell.execute_reply.started":"2024-11-14T09:43:14.888791Z","shell.execute_reply":"2024-11-14T09:43:27.587822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import stanza\nstanza.download('ar')  # Download the Arabic model\n# Load Stanza pipeline for Arabic\nnlp = stanza.Pipeline('ar', processors='tokenize,pos,ner')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:43:27.590750Z","iopub.execute_input":"2024-11-14T09:43:27.591582Z","iopub.status.idle":"2024-11-14T09:43:43.317132Z","shell.execute_reply.started":"2024-11-14T09:43:27.591530Z","shell.execute_reply":"2024-11-14T09:43:43.316234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('kaggle/input/augoriginal'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# load the tweets dataset\ndf_tweets = pd.read_csv('/kaggle/input/combinds/combinedDS_1.csv')\n# load the test dataset\ndf_tweets_test = pd.read_csv('/kaggle/input/val-test/testDS_1.csv')\n# load the validation dataset\ndf_tweets_val = pd.read_csv('/kaggle/input/val-test/task2_dev.csv') ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:43:43.319909Z","iopub.execute_input":"2024-11-14T09:43:43.320565Z","iopub.status.idle":"2024-11-14T09:43:44.076583Z","shell.execute_reply.started":"2024-11-14T09:43:43.320519Z","shell.execute_reply":"2024-11-14T09:43:44.075428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_tweets['technique'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:43:44.079027Z","iopub.execute_input":"2024-11-14T09:43:44.079866Z","iopub.status.idle":"2024-11-14T09:43:44.104550Z","shell.execute_reply.started":"2024-11-14T09:43:44.079813Z","shell.execute_reply":"2024-11-14T09:43:44.103288Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndf_tweets.drop(columns=[\"Unnamed: 0\", \"start\", \"end\",\"text\", \"start_new\",\"end_new\"], inplace=True)\n\n#df_tweets_test['augmented_text'] = df_tweets_test['text']\n#df_tweets_test.drop(columns=[\"start\", \"end\",\"Unnamed: 0\", \"text\", \"start_new\",\"end_new\"], inplace=True)\n\ndf_tweets_val['augmented_text'] = df_tweets_val['text']\ndf_tweets_val['augmented_full_text'] = df_tweets_val['full_text']\ndf_tweets_val.drop(columns=[\"start\", \"end\",\"Unnamed: 0\", \"text\", \"start_new\",\"end_new\"], inplace=True)\n\n\ndf_all_tweets_list = [df_tweets,df_tweets_val]\ndf_all_tweets = pd.concat(df_all_tweets_list)\n\n\n# First group tags Id wise\ndf_tags = df_all_tweets[['technique','augmented_full_text', 'augmented_text']]\nclasses = df_tags['technique'].unique()\n\n'''\ndf_tags = df_tags.drop_duplicates(subset=['technique','augmented_full_text'], ignore_index = True)\ndf_tags = df_tags.groupby('augmented_full_text').apply(lambda x:x['technique'].values).reset_index(name='techniques')\n'''\n\n# Group by 'augmented_full_text' and aggregate 'technique' and 'augmented_text' as lists\ngrouped_df = df_tags.groupby('augmented_full_text').agg({\n    'technique': list,\n    'augmented_text': list\n}).reset_index()\n\n'''\n# Define a regular expression pattern to match non-Arabic characters\nnon_arabic_pattern = r'[^\\u0600-\\u06FF\\s]'\n\n# Remove non-Arabic characters using the regular expression pattern\ndf_tags['augmented_full_text'] = df_tags['augmented_full_text'].str.replace(non_arabic_pattern, '', regex=True)\n\ndf_tags.shape\n'''\ndf_new = grouped_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:43:44.106067Z","iopub.execute_input":"2024-11-14T09:43:44.106471Z","iopub.status.idle":"2024-11-14T09:43:46.826134Z","shell.execute_reply.started":"2024-11-14T09:43:44.106434Z","shell.execute_reply":"2024-11-14T09:43:46.825097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:43:46.827639Z","iopub.execute_input":"2024-11-14T09:43:46.828090Z","iopub.status.idle":"2024-11-14T09:43:46.834964Z","shell.execute_reply.started":"2024-11-14T09:43:46.828025Z","shell.execute_reply":"2024-11-14T09:43:46.833932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\n\n# Fit and transform labels using MultiLabelBinarizer\nmlb = MultiLabelBinarizer(classes = classes)\n\nlabels = df_new['technique'].tolist()\n\nlabels_encoded = mlb.fit_transform(labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:43:46.836292Z","iopub.execute_input":"2024-11-14T09:43:46.836619Z","iopub.status.idle":"2024-11-14T09:43:48.320778Z","shell.execute_reply.started":"2024-11-14T09:43:46.836586Z","shell.execute_reply":"2024-11-14T09:43:48.319837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x = df_new['augmented_full_text'].values\ny = labels_encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:43:48.321932Z","iopub.execute_input":"2024-11-14T09:43:48.322466Z","iopub.status.idle":"2024-11-14T09:43:48.326960Z","shell.execute_reply.started":"2024-11-14T09:43:48.322429Z","shell.execute_reply":"2024-11-14T09:43:48.326012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# First group tags Id wise\ndf_tweets_test['augmented_full_text'] = df_tweets_test['full_text']\n#df_tweets_test.drop(columns=[\"start\", \"end\",\"Unnamed: 0\", \"text\", \"start_new\",\"end_new\"], inplace=True)\ntest_tags = df_tweets_test[['technique','augmented_full_text']]\ntest_classes = test_tags['technique'].unique()\ntest_tags = test_tags.drop_duplicates(subset=['technique','augmented_full_text'], ignore_index = True)\ntest_tags = test_tags.groupby('augmented_full_text').apply(lambda x:x['technique'].values).reset_index(name='technique')\n\n\n# Define a regular expression pattern to match non-Arabic characters\nnon_arabic_pattern = r'[^\\u0600-\\u06FF\\s]'\n\n# Remove non-Arabic characters using the regular expression pattern\ntest_tags['augmented_full_text'] = test_tags['augmented_full_text'].str.replace(non_arabic_pattern, '', regex=True)\n\n\ntest_labels = test_tags['technique'].tolist()\n\ntest_labels_encoded = mlb.fit_transform(test_labels)\n\n#df_tweets_test['augmented_text'] = df_tweets_test['text']\n#df_tweets_test.drop(columns=[\"start\", \"end\",\"Unnamed: 0\", \"full_text\" , \"text\", \"start_new\",\"end_new\"], inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:43:48.330701Z","iopub.execute_input":"2024-11-14T09:43:48.331056Z","iopub.status.idle":"2024-11-14T09:43:48.350864Z","shell.execute_reply.started":"2024-11-14T09:43:48.331004Z","shell.execute_reply":"2024-11-14T09:43:48.349940Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nx_ts = test_tags['augmented_full_text'].values\ny_ts = test_labels_encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:43:48.352000Z","iopub.execute_input":"2024-11-14T09:43:48.352518Z","iopub.status.idle":"2024-11-14T09:43:48.357816Z","shell.execute_reply.started":"2024-11-14T09:43:48.352477Z","shell.execute_reply":"2024-11-14T09:43:48.356855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Define your random seed\nRANDOM_SEED = 42\n\n# First, split into training + validation and test sets\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=RANDOM_SEED)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:43:48.358964Z","iopub.execute_input":"2024-11-14T09:43:48.359293Z","iopub.status.idle":"2024-11-14T09:43:48.450903Z","shell.execute_reply.started":"2024-11-14T09:43:48.359261Z","shell.execute_reply":"2024-11-14T09:43:48.449975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install transformers\nfrom transformers import TFBertModel\n# Load the BERT model\n#bert_model = TFBertModel.from_pretrained('aubmindlab/bert-base-arabertv02')\n#bert_model = TFBertModel.from_pretrained('aubmindlab/bert-base-arabertv02')\n# Define the hyperparameters\nmax_seq_length = 100\nnum_classes = len(classes)\nLEARNING_RATE = 2e-5   #0.001 => BERT  \nEPOCHS = 100\nBATCH_SIZE = 64\n\nSTEPS_PER_EPOCH = len(x_train)/BATCH_SIZE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:43:48.452250Z","iopub.execute_input":"2024-11-14T09:43:48.452938Z","iopub.status.idle":"2024-11-14T09:44:02.115495Z","shell.execute_reply.started":"2024-11-14T09:43:48.452893Z","shell.execute_reply":"2024-11-14T09:44:02.114698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizer\n# Tokenize and encode the training and testing data\ntokenizer = BertTokenizer.from_pretrained('aubmindlab/bert-base-arabertv02')\n# Tokenize and encode the training and testing data\n\n# Tokenize and encode the training data\ntrain_encodings = tokenizer(\n    x_train.tolist(),  # Convert x_train to a list of strings\n    truncation=True,\n    padding='max_length',\n    max_length=max_seq_length,\n    return_tensors='tf'  # Return TensorFlow tensors\n)\n\n# Tokenize and encode the training data\nval_encodings = tokenizer(\n    x_val.tolist(),  # Convert x_train to a list of strings\n    truncation=True,\n    padding='max_length',\n    max_length=max_seq_length,\n    return_tensors='tf'  # Return TensorFlow tensors\n)\n\n\n# Tokenize and encode the training data\ntest_encodings = tokenizer(\n    x_ts.tolist(),  # Convert x_train to a list of strings\n    truncation=True,\n    padding='max_length',\n    max_length=max_seq_length,\n    return_tensors='tf'  # Return TensorFlow tensors\n)\n\n# Define inputs for BERT tokenizer output\ntrain_inputs = [train_encodings['input_ids'], train_encodings['attention_mask'], train_encodings['token_type_ids']]\nval_inputs = [val_encodings['input_ids'], val_encodings['attention_mask'], val_encodings['token_type_ids']]\ntest_inputs = [test_encodings['input_ids'], test_encodings['attention_mask'], test_encodings['token_type_ids']]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:44:02.116633Z","iopub.execute_input":"2024-11-14T09:44:02.117288Z","iopub.status.idle":"2024-11-14T09:44:16.905630Z","shell.execute_reply.started":"2024-11-14T09:44:02.117252Z","shell.execute_reply":"2024-11-14T09:44:16.904800Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_pos_ner_features_stanza(texts):\n    pos_features = []\n    ner_features = []\n    for text in texts:\n        doc = nlp(text)\n        pos_tags = [word.upos for sent in doc.sentences for word in sent.words]  # POS tags\n        entities = [ent.type for ent in doc.ents]  # NER tags\n        pos_features.append(pos_tags)\n        ner_features.append(entities)\n    return pos_features, ner_features\n\n# Get POS and NER features for your training, validation, and test sets\npos_train, ner_train = get_pos_ner_features_stanza(x_train)\npos_val, ner_val = get_pos_ner_features_stanza(x_val)\npos_test, ner_test = get_pos_ner_features_stanza(x_ts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:44:16.906753Z","iopub.execute_input":"2024-11-14T09:44:16.907053Z","iopub.status.idle":"2024-11-14T10:11:54.988023Z","shell.execute_reply.started":"2024-11-14T09:44:16.907010Z","shell.execute_reply":"2024-11-14T10:11:54.987060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenize POS + NER features (Ensure tokenizer is defined as earlier)\npos_train_sequences = tokenizer.batch_encode_plus([\" \".join(tags) for tags in pos_train], max_length=max_seq_length, truncation=True, padding='max_length', return_tensors='tf')\nner_train_sequences = tokenizer.batch_encode_plus([\" \".join(tags) for tags in ner_train], max_length=max_seq_length, truncation=True, padding='max_length', return_tensors='tf')\npos_val_sequences = tokenizer.batch_encode_plus([\" \".join(tags) for tags in pos_val], max_length=max_seq_length, truncation=True, padding='max_length', return_tensors='tf')\nner_val_sequences = tokenizer.batch_encode_plus([\" \".join(tags) for tags in ner_val], max_length=max_seq_length, truncation=True, padding='max_length', return_tensors='tf')\npos_test_sequences = tokenizer.batch_encode_plus([\" \".join(tags) for tags in pos_test], max_length=max_seq_length, truncation=True, padding='max_length', return_tensors='tf')\nner_test_sequences = tokenizer.batch_encode_plus([\" \".join(tags) for tags in ner_test], max_length=max_seq_length, truncation=True, padding='max_length', return_tensors='tf')\n\n# POS and NER inputs (tokenized sequences)\npos_train_input = pos_train_sequences['input_ids']\nner_train_input = ner_train_sequences['input_ids']\npos_val_input = pos_val_sequences['input_ids']\nner_val_input = ner_val_sequences['input_ids']\npos_test_input = pos_test_sequences['input_ids']\nner_test_input = ner_test_sequences['input_ids']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T10:11:54.989523Z","iopub.execute_input":"2024-11-14T10:11:54.990374Z","iopub.status.idle":"2024-11-14T10:12:13.459892Z","shell.execute_reply.started":"2024-11-14T10:11:54.990318Z","shell.execute_reply":"2024-11-14T10:12:13.459096Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_inputs = [train_encodings['input_ids'], train_encodings['attention_mask'], train_encodings['token_type_ids']]\ntest_inputs = [test_encodings['input_ids'], test_encodings['attention_mask'], test_encodings['token_type_ids']]\nval_inputs = [val_encodings['input_ids'], val_encodings['attention_mask'], val_encodings['token_type_ids']]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T10:12:13.461276Z","iopub.execute_input":"2024-11-14T10:12:13.462026Z","iopub.status.idle":"2024-11-14T10:12:13.467912Z","shell.execute_reply.started":"2024-11-14T10:12:13.461977Z","shell.execute_reply":"2024-11-14T10:12:13.466972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pathlib\nimport shutil\nimport tempfile\n\nlogdir = '/kaggle/working/logdir/tensorboard_logs_new'\nshutil.rmtree(logdir, ignore_errors=True)\n\ncheckpoint_path = \"/kaggle/working/checkpoint/training_2/cp-{epoch:04d}.ckpt\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T10:12:13.468986Z","iopub.execute_input":"2024-11-14T10:12:13.469296Z","iopub.status.idle":"2024-11-14T10:12:13.480617Z","shell.execute_reply.started":"2024-11-14T10:12:13.469265Z","shell.execute_reply":"2024-11-14T10:12:13.479903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef get_callbacks(name):\n  return [\n    tfdocs.modeling.EpochDots(),\n    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n    tf.keras.callbacks.TensorBoard(logdir+'/'+name),\n    tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_path,\n    verbose=1,\n    save_weights_only=True,\n    save_freq='epoch',\n    period=30)\n  ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T10:12:13.482009Z","iopub.execute_input":"2024-11-14T10:12:13.482470Z","iopub.status.idle":"2024-11-14T10:12:13.492233Z","shell.execute_reply.started":"2024-11-14T10:12:13.482427Z","shell.execute_reply":"2024-11-14T10:12:13.491363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFBertModel\n\n# Define maximum sequence length\nmax_seq_length = 100  # Adjust according to your model/data\n\n# Define the number of classes for your classification task\nnum_classes = 18  # Adjust based on your dataset\n\ndef create_model():\n    # Load BERT model from Hugging Face\n    bert_model = TFBertModel.from_pretrained('aubmindlab/bert-base-arabertv02')\n\n    # Input layers for BERT\n    input_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"attention_mask\")\n    token_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"token_type_ids\")\n    \n    # BERT Model Outputs\n    bert_outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n    pooled_output = bert_outputs.pooler_output  # [CLS] token output for classification tasks\n\n    # POS and NER Inputs\n    pos_input = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"pos_input\")\n    ner_input = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"ner_input\")\n\n    # Embedding layers for POS and NER inputs\n    pos_embedding = tf.keras.layers.Embedding(input_dim=50, output_dim=16, input_length=max_seq_length)(pos_input)\n    ner_embedding = tf.keras.layers.Embedding(input_dim=50, output_dim=16, input_length=max_seq_length)(ner_input)\n\n    # Flatten the embeddings\n    pos_flat = tf.keras.layers.Flatten()(pos_embedding)\n    ner_flat = tf.keras.layers.Flatten()(ner_embedding)\n\n    # Concatenate BERT output with POS and NER embeddings\n    combined_features = tf.keras.layers.Concatenate(axis=-1)([pooled_output, pos_flat, ner_flat])\n\n    # Fully connected hidden layers\n    hidden_layer1 = tf.keras.layers.Dense(256, activation='relu')(combined_features)\n    hidden_layer2 = tf.keras.layers.Dense(128, activation='relu')(hidden_layer1)\n    hidden_layer3 = tf.keras.layers.Dense(64, activation='relu')(hidden_layer2)\n\n\n    # Output layer with sigmoid activation for multi-label classification\n    output = tf.keras.layers.Dense(num_classes, activation='sigmoid')(hidden_layer3)\n\n    # Define the model\n    model = tf.keras.Model(\n        inputs=[input_ids, attention_mask, token_type_ids, pos_input, ner_input],\n        outputs=output\n    )\n\n    return model\n\n# Create the model\nmodel = create_model()\n\n# Compile the model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Display the model summary to verify the architecture\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T10:12:13.493298Z","iopub.execute_input":"2024-11-14T10:12:13.493586Z","iopub.status.idle":"2024-11-14T10:12:43.638514Z","shell.execute_reply.started":"2024-11-14T10:12:13.493556Z","shell.execute_reply":"2024-11-14T10:12:43.637755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n# Define the weighted loss function\n#loss = tf.keras.losses.BinaryCrossentropy(from_logits=False, pos_weight=18)\nloss= 'binary_crossentropy'\n#loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\naccuracy=tf.keras.metrics.CategoricalAccuracy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T10:12:43.641465Z","iopub.execute_input":"2024-11-14T10:12:43.641764Z","iopub.status.idle":"2024-11-14T10:12:43.655022Z","shell.execute_reply.started":"2024-11-14T10:12:43.641732Z","shell.execute_reply":"2024-11-14T10:12:43.653915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compile_and_fit(model_name, optimizer, loss_fn, accuracy, name, epochs_num, batch_size, steps_per_epoch):\n\n\n\n\n    model_name.compile(optimizer=optimizer,\n                loss=loss_fn,\n                metrics=[\n                  tf.keras.metrics.BinaryCrossentropy(\n                      from_logits=False, name='binary_crossentropy'),\n                  'accuracy'])\n\n    model_name.summary()\n\n    history = model.fit({\n        'input_ids': train_inputs[0],\n        'attention_mask': train_inputs[1],\n        'token_type_ids': train_inputs[2],\n        'pos_input': pos_train_input,\n        'ner_input': ner_train_input\n    },y_train,\n                  validation_data=({\n            'input_ids': val_inputs[0],\n            'attention_mask': val_inputs[1],\n            'token_type_ids': val_inputs[2],\n            'pos_input': pos_val_input,\n            'ner_input': ner_val_input\n        },y_val),\n                        epochs=epochs_num,batch_size=batch_size,steps_per_epoch = steps_per_epoch,\n                        callbacks=get_callbacks(name),\n                        verbose=1)\n\n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T10:12:43.656392Z","iopub.execute_input":"2024-11-14T10:12:43.656679Z","iopub.status.idle":"2024-11-14T10:12:43.665692Z","shell.execute_reply.started":"2024-11-14T10:12:43.656648Z","shell.execute_reply":"2024-11-14T10:12:43.664708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"size_histories = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T10:12:43.666973Z","iopub.execute_input":"2024-11-14T10:12:43.667331Z","iopub.status.idle":"2024-11-14T10:12:43.681665Z","shell.execute_reply.started":"2024-11-14T10:12:43.667294Z","shell.execute_reply":"2024-11-14T10:12:43.680715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install git+https://github.com/tensorflow/docs\nimport tensorflow_docs as tfdocs\nimport tensorflow_docs.modeling\nimport tensorflow_docs.plots","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T10:12:43.682815Z","iopub.execute_input":"2024-11-14T10:12:43.683138Z","iopub.status.idle":"2024-11-14T10:13:01.886909Z","shell.execute_reply.started":"2024-11-14T10:12:43.683104Z","shell.execute_reply":"2024-11-14T10:13:01.885698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"size_histories['ORIGINAL'] = compile_and_fit(model, optimizer, loss, accuracy, 'ORIGINAL', EPOCHS, BATCH_SIZE, STEPS_PER_EPOCH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T10:13:01.888478Z","iopub.execute_input":"2024-11-14T10:13:01.888812Z","iopub.status.idle":"2024-11-14T11:12:52.093591Z","shell.execute_reply.started":"2024-11-14T10:13:01.888776Z","shell.execute_reply":"2024-11-14T11:12:52.092658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plotter = tfdocs.plots.HistoryPlotter(metric = 'binary_crossentropy', smoothing_std=10)\nplotter.plot(size_histories)\n\n#plt.ylim([-1, 2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:12:52.094936Z","iopub.execute_input":"2024-11-14T11:12:52.095300Z","iopub.status.idle":"2024-11-14T11:12:52.438440Z","shell.execute_reply.started":"2024-11-14T11:12:52.095264Z","shell.execute_reply":"2024-11-14T11:12:52.437428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure you're passing the correct input format for predictions (similar to training)\ntest_scores = model.predict({\n    'input_ids': test_inputs[0],\n    'attention_mask': test_inputs[1],\n    'token_type_ids': test_inputs[2],\n    'pos_input': pos_test_input,\n    'ner_input': ner_test_input\n})\n\n# Convert scores to binary labels (0 or 1) based on a threshold of 0.5\ntest_pred_labels = (test_scores >= 0.5).astype(int)\n\n# True labels\ntest_true_labels = y_ts.astype(int)\n\n# Optionally, evaluate accuracy or other metrics\nfrom sklearn.metrics import accuracy_score, f1_score, hamming_loss\n\n# Calculate accuracy, F1 score, and Hamming loss\naccuracy = accuracy_score(test_true_labels, test_pred_labels)\nf1 = f1_score(test_true_labels, test_pred_labels, average='micro')\nhamming = hamming_loss(test_true_labels, test_pred_labels)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"F1 Score (micro): {f1}\")\nprint(f\"Hamming Loss: {hamming}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:12:52.439636Z","iopub.execute_input":"2024-11-14T11:12:52.439945Z","iopub.status.idle":"2024-11-14T11:13:05.107948Z","shell.execute_reply.started":"2024-11-14T11:12:52.439911Z","shell.execute_reply":"2024-11-14T11:13:05.106975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, recall_score,classification_report\n\n# Calculate F1 score\nf1_macro = f1_score(test_true_labels, test_pred_labels, average='macro')\nf1_micro = f1_score(test_true_labels, test_pred_labels, average='micro')\n\n# Calculate confusion matrix, accuracy, and recall\nconfusion_mat = confusion_matrix(test_true_labels.flatten(), test_pred_labels.flatten())\naccuracy = accuracy_score(test_true_labels.flatten(), test_pred_labels.flatten())\nrecall = recall_score(test_true_labels.flatten(), test_pred_labels.flatten(), average='macro')\nclassification_rep = classification_report(test_true_labels.flatten(), test_pred_labels.flatten())\n\nfrom sklearn import metrics\n\nprint(\"Classification Report:\")\nprint(classification_rep)\nprint(\"F1 macro score:\", f1_macro)\nprint(\"F1 micro score:\", f1_micro)\nprint(\"Confusion Matrix:\")\nprint(confusion_mat)\nprint(\"Accuracy:\", accuracy)\nprint(\"Recall:\", recall)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:13:05.117619Z","iopub.execute_input":"2024-11-14T11:13:05.118014Z","iopub.status.idle":"2024-11-14T11:13:05.144408Z","shell.execute_reply.started":"2024-11-14T11:13:05.117981Z","shell.execute_reply":"2024-11-14T11:13:05.143610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Function to map binary labels to actual label names\ndef binary_to_text_labels(binary_labels, class_names):\n    return [class_names[i] for i in range(len(binary_labels)) if binary_labels[i] == 1]\n\n# Create a list to store the results\nresults = []\n\n# Iterate over the test samples and collect the text, actual labels, and predicted labels\nfor i in range(len(x_ts)):  # test_texts is your list of test samples\n    actual_label_names = binary_to_text_labels(test_true_labels[i], classes)  # using 'classes' array\n    predicted_label_names = binary_to_text_labels(test_pred_labels[i], classes)  # using 'classes' array\n\n    # Append the results to the list\n    results.append({\n        \"Text\": x_ts[i],\n        \"Actual Labels\": ', '.join(actual_label_names),\n        \"Predicted Labels\": ', '.join(predicted_label_names)\n    })\n\n# Convert the results list to a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\n# Display the DataFrame\nresults_df  # Use .head() to display the first few rows\n\n# If you want to display the entire table, you can use:\n# pd.set_option('display.max_rows', None)  # Uncomment this line to display all rows if needed\n# display(results_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:13:05.145363Z","iopub.execute_input":"2024-11-14T11:13:05.145626Z","iopub.status.idle":"2024-11-14T11:13:05.171951Z","shell.execute_reply.started":"2024-11-14T11:13:05.145597Z","shell.execute_reply":"2024-11-14T11:13:05.171110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the model to a file\nmodel.save(\"/kaggle/working/predict_resuls/BERT_2/combinedDS/combinedDS.h5\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:13:05.173269Z","iopub.execute_input":"2024-11-14T11:13:05.173927Z","iopub.status.idle":"2024-11-14T11:13:08.394302Z","shell.execute_reply.started":"2024-11-14T11:13:05.173883Z","shell.execute_reply":"2024-11-14T11:13:08.393457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the results to a CSV file\nresults_df.to_csv(\"/kaggle/working/predict_resuls/BERT_2/combinedDS/results_df.csv\", index=False)\n\n# Alternatively, save to an Excel file\nresults_df.to_excel(\"/kaggle/working/predict_resuls/BERT_2/combinedDS/results_df.xlsx\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:13:08.395594Z","iopub.execute_input":"2024-11-14T11:13:08.395901Z","iopub.status.idle":"2024-11-14T11:13:08.691174Z","shell.execute_reply.started":"2024-11-14T11:13:08.395868Z","shell.execute_reply":"2024-11-14T11:13:08.690436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save evaluation metrics to a text file\nwith open(\"/kaggle/working/predict_resuls/BERT_2/combinedDS/evaluation_metrics.txt\", \"w\") as file:\n    file.write(\"Classification Report:\\n\")\n    file.write(classification_rep)\n    file.write(f\"\\nF1 macro score: {f1_macro}\\n\")\n    file.write(f\"F1 micro score: {f1_micro}\\n\")\n    file.write(f\"\\nConfusion Matrix:\\n{confusion_mat}\\n\")\n    file.write(f\"Accuracy: {accuracy}\\n\")\n    file.write(f\"Recall: {recall}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:13:08.692319Z","iopub.execute_input":"2024-11-14T11:13:08.692881Z","iopub.status.idle":"2024-11-14T11:13:08.698820Z","shell.execute_reply.started":"2024-11-14T11:13:08.692844Z","shell.execute_reply":"2024-11-14T11:13:08.697812Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_loss(size_histories):\n    plt.figure(figsize=(10, 6))\n\n    # Assuming 'ORIGINAL' is the key you want to plot\n    key = 'ORIGINAL'\n\n    history = size_histories[key]\n    plt.plot(history.history['val_loss'], label=f'{key} Validation Loss')\n    plt.plot(history.history['loss'], label=f'{key} Training Loss')\n\n    plt.title(\"Training and Validation Loss over Epochs\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_loss(size_histories)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:13:08.699897Z","iopub.execute_input":"2024-11-14T11:13:08.700221Z","iopub.status.idle":"2024-11-14T11:13:09.027717Z","shell.execute_reply.started":"2024-11-14T11:13:08.700184Z","shell.execute_reply":"2024-11-14T11:13:09.026787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_and_save_size_histories(size_histories, filename):\n    plt.figure(figsize=(10, 6))\n\n    for key, history in size_histories.items():\n        plt.plot(history.history['val_loss'], label=f'{key} Validation Loss')\n        plt.plot(history.history['loss'], '--', label=f'{key} Training Loss')\n\n    plt.title(\"Training and Validation Loss over Epochs\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n\n    # Save the plot to a file\n    plt.savefig(filename)\n    plt.close()  # Close the plot to prevent displaying it if running in a loop\n\n# Save the plot\nplot_and_save_size_histories(size_histories, filename=\"/kaggle/working/predict_resuls/BERT_2/combinedDS/Training and Validation Loss over Epochs.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:13:09.028890Z","iopub.execute_input":"2024-11-14T11:13:09.029236Z","iopub.status.idle":"2024-11-14T11:13:09.242982Z","shell.execute_reply.started":"2024-11-14T11:13:09.029200Z","shell.execute_reply":"2024-11-14T11:13:09.242026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training and validation accuracy\ndef plot_accuracy(size_histories):\n    plt.figure(figsize=(10, 6))\n\n    for key, history in size_histories.items():\n        plt.plot(history.history['val_accuracy'], label=f'{key} Validation Accuracy')\n        plt.plot(history.history['accuracy'], '--', label=f'{key} Training Accuracy')\n\n    plt.title(\"Training and Validation Accuracy over Epochs\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_accuracy(size_histories)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:13:09.244698Z","iopub.execute_input":"2024-11-14T11:13:09.245166Z","iopub.status.idle":"2024-11-14T11:13:09.498092Z","shell.execute_reply.started":"2024-11-14T11:13:09.245122Z","shell.execute_reply":"2024-11-14T11:13:09.497159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_and_save_size_histories(size_histories, filename):\n    plt.figure(figsize=(10, 6))\n\n    for key, history in size_histories.items():\n        plt.plot(history.history['val_accuracy'], label=f'{key} Validation Accuracy')\n        plt.plot(history.history['accuracy'], '--', label=f'{key} Training Accuracy')\n\n    plt.title(\"Training and Validation Accuracy over Epochs\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.grid(True)\n\n\n    # Save the plot to a file\n    plt.savefig(filename)\n    plt.close()  # Close the plot to prevent displaying it if running in a loop\n\n# Save the plot\nplot_and_save_size_histories(size_histories, filename=\"/kaggle/working/predict_resuls/BERT_2/combinedDS/Training and Validation Accuracy over Epochs.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:13:09.499512Z","iopub.execute_input":"2024-11-14T11:13:09.499952Z","iopub.status.idle":"2024-11-14T11:13:09.757625Z","shell.execute_reply.started":"2024-11-14T11:13:09.499901Z","shell.execute_reply":"2024-11-14T11:13:09.756844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training and validation binary crossentropy\ndef plot_binary_crossentropy(size_histories):\n    plt.figure(figsize=(10, 6))\n\n    for key, history in size_histories.items():\n        plt.plot(history.history['val_binary_crossentropy'], label=f'{key} Validation Binary Crossentropy')\n        plt.plot(history.history['binary_crossentropy'], '--', label=f'{key} Training Binary Crossentropy')\n\n    plt.title(\"Training and Validation Binary Crossentropy over Epochs\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Binary Crossentropy\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_binary_crossentropy(size_histories)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:13:09.758717Z","iopub.execute_input":"2024-11-14T11:13:09.759066Z","iopub.status.idle":"2024-11-14T11:13:10.090400Z","shell.execute_reply.started":"2024-11-14T11:13:09.759007Z","shell.execute_reply":"2024-11-14T11:13:10.089409Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_and_save_size_histories(size_histories, filename):\n    plt.figure(figsize=(10, 6))\n\n    for key, history in size_histories.items():\n        plt.plot(history.history['val_binary_crossentropy'], label=f'{key} Validation Binary Crossentropy')\n        plt.plot(history.history['binary_crossentropy'], '--', label=f'{key} Training Binary Crossentropy')\n\n    plt.title(\"Training and Validation Binary Crossentropy over Epochs\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Binary Crossentropy\")\n    plt.legend()\n    plt.grid(True)\n\n    # Save the plot to a file\n    plt.savefig(filename)\n    plt.close()  # Close the plot to prevent displaying it if running in a loop\n\n# Save the plot\nplot_and_save_size_histories(size_histories, filename=\"/kaggle/working/predict_resuls/BERT_2/combinedDS/Training and Validation Binary Crossentropy over Epochs.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:13:10.091518Z","iopub.execute_input":"2024-11-14T11:13:10.091798Z","iopub.status.idle":"2024-11-14T11:13:10.308233Z","shell.execute_reply.started":"2024-11-14T11:13:10.091768Z","shell.execute_reply":"2024-11-14T11:13:10.307310Z"}},"outputs":[],"execution_count":null}]}