{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9893720,"sourceType":"datasetVersion","datasetId":6076642},{"sourceId":9893901,"sourceType":"datasetVersion","datasetId":6076769}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install transformers\n!pip install transformers tensorflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:47:51.384417Z","iopub.execute_input":"2024-11-14T11:47:51.385695Z","iopub.status.idle":"2024-11-14T11:48:06.877388Z","shell.execute_reply.started":"2024-11-14T11:47:51.385634Z","shell.execute_reply":"2024-11-14T11:48:06.876097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install stanza","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:48:06.879593Z","iopub.execute_input":"2024-11-14T11:48:06.879939Z","iopub.status.idle":"2024-11-14T11:48:21.724141Z","shell.execute_reply.started":"2024-11-14T11:48:06.879901Z","shell.execute_reply":"2024-11-14T11:48:21.723078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import stanza\nstanza.download('ar')  # Download the Arabic model\n# Load Stanza pipeline for Arabic\nnlp = stanza.Pipeline('ar', processors='tokenize,pos,ner')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:48:21.725603Z","iopub.execute_input":"2024-11-14T11:48:21.725936Z","iopub.status.idle":"2024-11-14T11:48:40.957805Z","shell.execute_reply.started":"2024-11-14T11:48:21.725899Z","shell.execute_reply":"2024-11-14T11:48:40.956357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('kaggle/input/augoriginal'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# load the tweets dataset\ndf_tweets = pd.read_csv('/kaggle/input/augoriginal/augOriginal_1.csv')\n# load the test dataset\ndf_tweets_test = pd.read_csv('/kaggle/input/val-test/testDS_1.csv')\n# load the validation dataset\ndf_tweets_val = pd.read_csv('/kaggle/input/val-test/task2_dev.csv') ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:48:40.960940Z","iopub.execute_input":"2024-11-14T11:48:40.961999Z","iopub.status.idle":"2024-11-14T11:48:41.512099Z","shell.execute_reply.started":"2024-11-14T11:48:40.961941Z","shell.execute_reply":"2024-11-14T11:48:41.510766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_tweets['technique'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:48:41.514012Z","iopub.execute_input":"2024-11-14T11:48:41.514555Z","iopub.status.idle":"2024-11-14T11:48:42.519124Z","shell.execute_reply.started":"2024-11-14T11:48:41.514500Z","shell.execute_reply":"2024-11-14T11:48:42.517918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndf_tweets.drop(columns=[\"Unnamed: 0\", \"start\", \"end\",\"text\", \"start_new\",\"end_new\"], inplace=True)\n\n#df_tweets_test['augmented_text'] = df_tweets_test['text']\n#df_tweets_test.drop(columns=[\"start\", \"end\",\"Unnamed: 0\", \"text\", \"start_new\",\"end_new\"], inplace=True)\n\ndf_tweets_val['augmented_text'] = df_tweets_val['text']\ndf_tweets_val['augmented_full_text'] = df_tweets_val['full_text']\ndf_tweets_val.drop(columns=[\"start\", \"end\",\"Unnamed: 0\", \"text\", \"start_new\",\"end_new\"], inplace=True)\n\n\ndf_all_tweets_list = [df_tweets,df_tweets_val]\ndf_all_tweets = pd.concat(df_all_tweets_list)\n\n\n# First group tags Id wise\ndf_tags = df_all_tweets[['technique','augmented_full_text', 'augmented_text']]\nclasses = df_tags['technique'].unique()\n\n'''\ndf_tags = df_tags.drop_duplicates(subset=['technique','augmented_full_text'], ignore_index = True)\ndf_tags = df_tags.groupby('augmented_full_text').apply(lambda x:x['technique'].values).reset_index(name='techniques')\n'''\n\n# Group by 'augmented_full_text' and aggregate 'technique' and 'augmented_text' as lists\ngrouped_df = df_tags.groupby('augmented_full_text').agg({\n    'technique': list,\n    'augmented_text': list\n}).reset_index()\n\n'''\n# Define a regular expression pattern to match non-Arabic characters\nnon_arabic_pattern = r'[^\\u0600-\\u06FF\\s]'\n\n# Remove non-Arabic characters using the regular expression pattern\ndf_tags['augmented_full_text'] = df_tags['augmented_full_text'].str.replace(non_arabic_pattern, '', regex=True)\n\ndf_tags.shape\n'''\ndf_new = grouped_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:48:42.520642Z","iopub.execute_input":"2024-11-14T11:48:42.521126Z","iopub.status.idle":"2024-11-14T11:48:42.862543Z","shell.execute_reply.started":"2024-11-14T11:48:42.521072Z","shell.execute_reply":"2024-11-14T11:48:42.861302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:48:42.863956Z","iopub.execute_input":"2024-11-14T11:48:42.864393Z","iopub.status.idle":"2024-11-14T11:48:42.872215Z","shell.execute_reply.started":"2024-11-14T11:48:42.864349Z","shell.execute_reply":"2024-11-14T11:48:42.871116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\n\n# Fit and transform labels using MultiLabelBinarizer\nmlb = MultiLabelBinarizer(classes = classes)\n\nlabels = df_new['technique'].tolist()\n\nlabels_encoded = mlb.fit_transform(labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:48:42.873671Z","iopub.execute_input":"2024-11-14T11:48:42.874158Z","iopub.status.idle":"2024-11-14T11:48:43.631769Z","shell.execute_reply.started":"2024-11-14T11:48:42.874104Z","shell.execute_reply":"2024-11-14T11:48:43.630728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x = df_new['augmented_full_text'].values\ny = labels_encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:48:43.632992Z","iopub.execute_input":"2024-11-14T11:48:43.633635Z","iopub.status.idle":"2024-11-14T11:48:43.638985Z","shell.execute_reply.started":"2024-11-14T11:48:43.633595Z","shell.execute_reply":"2024-11-14T11:48:43.637916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# First group tags Id wise\ndf_tweets_test['augmented_full_text'] = df_tweets_test['full_text']\n#df_tweets_test.drop(columns=[\"start\", \"end\",\"Unnamed: 0\", \"text\", \"start_new\",\"end_new\"], inplace=True)\ntest_tags = df_tweets_test[['technique','augmented_full_text']]\ntest_classes = test_tags['technique'].unique()\ntest_tags = test_tags.drop_duplicates(subset=['technique','augmented_full_text'], ignore_index = True)\ntest_tags = test_tags.groupby('augmented_full_text').apply(lambda x:x['technique'].values).reset_index(name='technique')\n\n\n# Define a regular expression pattern to match non-Arabic characters\nnon_arabic_pattern = r'[^\\u0600-\\u06FF\\s]'\n\n# Remove non-Arabic characters using the regular expression pattern\ntest_tags['augmented_full_text'] = test_tags['augmented_full_text'].str.replace(non_arabic_pattern, '', regex=True)\n\n\ntest_labels = test_tags['technique'].tolist()\n\ntest_labels_encoded = mlb.fit_transform(test_labels)\n\n#df_tweets_test['augmented_text'] = df_tweets_test['text']\n#df_tweets_test.drop(columns=[\"start\", \"end\",\"Unnamed: 0\", \"full_text\" , \"text\", \"start_new\",\"end_new\"], inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:48:43.643173Z","iopub.execute_input":"2024-11-14T11:48:43.643559Z","iopub.status.idle":"2024-11-14T11:48:43.665778Z","shell.execute_reply.started":"2024-11-14T11:48:43.643521Z","shell.execute_reply":"2024-11-14T11:48:43.664546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nx_ts = test_tags['augmented_full_text'].values\ny_ts = test_labels_encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:48:43.667337Z","iopub.execute_input":"2024-11-14T11:48:43.667679Z","iopub.status.idle":"2024-11-14T11:48:43.673283Z","shell.execute_reply.started":"2024-11-14T11:48:43.667643Z","shell.execute_reply":"2024-11-14T11:48:43.672102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Define your random seed\nRANDOM_SEED = 42\n\n# First, split into training + validation and test sets\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=RANDOM_SEED)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:48:43.674748Z","iopub.execute_input":"2024-11-14T11:48:43.675136Z","iopub.status.idle":"2024-11-14T11:48:43.772169Z","shell.execute_reply.started":"2024-11-14T11:48:43.675094Z","shell.execute_reply":"2024-11-14T11:48:43.770924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install transformers\nfrom transformers import TFBertModel\n# Load the BERT model\n#bert_model = TFBertModel.from_pretrained('aubmindlab/bert-base-arabertv02')\n#bert_model = TFBertModel.from_pretrained('aubmindlab/bert-base-arabertv02')\n# Define the hyperparameters\nmax_seq_length = 100\nnum_classes = len(classes)\nLEARNING_RATE = 2e-5   #0.001 => BERT  \nEPOCHS = 100\nBATCH_SIZE = 64\n\nSTEPS_PER_EPOCH = len(x_train)/BATCH_SIZE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:48:43.773457Z","iopub.execute_input":"2024-11-14T11:48:43.773803Z","iopub.status.idle":"2024-11-14T11:48:58.487696Z","shell.execute_reply.started":"2024-11-14T11:48:43.773758Z","shell.execute_reply":"2024-11-14T11:48:58.486589Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizer\n# Tokenize and encode the training and testing data\ntokenizer = BertTokenizer.from_pretrained('aubmindlab/bert-base-arabertv02')\n# Tokenize and encode the training and testing data\n\n# Tokenize and encode the training data\ntrain_encodings = tokenizer(\n    x_train.tolist(),  # Convert x_train to a list of strings\n    truncation=True,\n    padding='max_length',\n    max_length=max_seq_length,\n    return_tensors='tf'  # Return TensorFlow tensors\n)\n\n# Tokenize and encode the training data\nval_encodings = tokenizer(\n    x_val.tolist(),  # Convert x_train to a list of strings\n    truncation=True,\n    padding='max_length',\n    max_length=max_seq_length,\n    return_tensors='tf'  # Return TensorFlow tensors\n)\n\n\n# Tokenize and encode the training data\ntest_encodings = tokenizer(\n    x_ts.tolist(),  # Convert x_train to a list of strings\n    truncation=True,\n    padding='max_length',\n    max_length=max_seq_length,\n    return_tensors='tf'  # Return TensorFlow tensors\n)\n\n# Define inputs for BERT tokenizer output\ntrain_inputs = [train_encodings['input_ids'], train_encodings['attention_mask'], train_encodings['token_type_ids']]\nval_inputs = [val_encodings['input_ids'], val_encodings['attention_mask'], val_encodings['token_type_ids']]\ntest_inputs = [test_encodings['input_ids'], test_encodings['attention_mask'], test_encodings['token_type_ids']]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:48:58.488984Z","iopub.execute_input":"2024-11-14T11:48:58.489728Z","iopub.status.idle":"2024-11-14T11:49:03.915049Z","shell.execute_reply.started":"2024-11-14T11:48:58.489686Z","shell.execute_reply":"2024-11-14T11:49:03.914111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_pos_ner_features_stanza(texts):\n    pos_features = []\n    ner_features = []\n    for text in texts:\n        doc = nlp(text)\n        pos_tags = [word.upos for sent in doc.sentences for word in sent.words]  # POS tags\n        entities = [ent.type for ent in doc.ents]  # NER tags\n        pos_features.append(pos_tags)\n        ner_features.append(entities)\n    return pos_features, ner_features\n\n# Get POS and NER features for your training, validation, and test sets\npos_train, ner_train = get_pos_ner_features_stanza(x_train)\npos_val, ner_val = get_pos_ner_features_stanza(x_val)\npos_test, ner_test = get_pos_ner_features_stanza(x_ts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:49:03.916452Z","iopub.execute_input":"2024-11-14T11:49:03.916817Z","iopub.status.idle":"2024-11-14T11:56:30.421631Z","shell.execute_reply.started":"2024-11-14T11:49:03.916777Z","shell.execute_reply":"2024-11-14T11:56:30.420133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenize POS + NER features (Ensure tokenizer is defined as earlier)\npos_train_sequences = tokenizer.batch_encode_plus([\" \".join(tags) for tags in pos_train], max_length=max_seq_length, truncation=True, padding='max_length', return_tensors='tf')\nner_train_sequences = tokenizer.batch_encode_plus([\" \".join(tags) for tags in ner_train], max_length=max_seq_length, truncation=True, padding='max_length', return_tensors='tf')\npos_val_sequences = tokenizer.batch_encode_plus([\" \".join(tags) for tags in pos_val], max_length=max_seq_length, truncation=True, padding='max_length', return_tensors='tf')\nner_val_sequences = tokenizer.batch_encode_plus([\" \".join(tags) for tags in ner_val], max_length=max_seq_length, truncation=True, padding='max_length', return_tensors='tf')\npos_test_sequences = tokenizer.batch_encode_plus([\" \".join(tags) for tags in pos_test], max_length=max_seq_length, truncation=True, padding='max_length', return_tensors='tf')\nner_test_sequences = tokenizer.batch_encode_plus([\" \".join(tags) for tags in ner_test], max_length=max_seq_length, truncation=True, padding='max_length', return_tensors='tf')\n\n# POS and NER inputs (tokenized sequences)\npos_train_input = pos_train_sequences['input_ids']\nner_train_input = ner_train_sequences['input_ids']\npos_val_input = pos_val_sequences['input_ids']\nner_val_input = ner_val_sequences['input_ids']\npos_test_input = pos_test_sequences['input_ids']\nner_test_input = ner_test_sequences['input_ids']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:56:30.423372Z","iopub.execute_input":"2024-11-14T11:56:30.424370Z","iopub.status.idle":"2024-11-14T11:56:35.229581Z","shell.execute_reply.started":"2024-11-14T11:56:30.424312Z","shell.execute_reply":"2024-11-14T11:56:35.228076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_inputs = [train_encodings['input_ids'], train_encodings['attention_mask'], train_encodings['token_type_ids']]\ntest_inputs = [test_encodings['input_ids'], test_encodings['attention_mask'], test_encodings['token_type_ids']]\nval_inputs = [val_encodings['input_ids'], val_encodings['attention_mask'], val_encodings['token_type_ids']]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:56:35.233120Z","iopub.execute_input":"2024-11-14T11:56:35.233562Z","iopub.status.idle":"2024-11-14T11:56:35.239378Z","shell.execute_reply.started":"2024-11-14T11:56:35.233522Z","shell.execute_reply":"2024-11-14T11:56:35.238297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pathlib\nimport shutil\nimport tempfile\n\nlogdir = '/kaggle/working/logdir/tensorboard_logs_new'\nshutil.rmtree(logdir, ignore_errors=True)\n\ncheckpoint_path = \"/kaggle/working/checkpoint/training_2/cp-{epoch:04d}.ckpt\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:56:35.240765Z","iopub.execute_input":"2024-11-14T11:56:35.241146Z","iopub.status.idle":"2024-11-14T11:56:35.252168Z","shell.execute_reply.started":"2024-11-14T11:56:35.241090Z","shell.execute_reply":"2024-11-14T11:56:35.251103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef get_callbacks(name):\n  return [\n    tfdocs.modeling.EpochDots(),\n    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n    tf.keras.callbacks.TensorBoard(logdir+'/'+name),\n    tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_path,\n    verbose=1,\n    save_weights_only=True,\n    save_freq='epoch',\n    period=30)\n  ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:56:35.253611Z","iopub.execute_input":"2024-11-14T11:56:35.254066Z","iopub.status.idle":"2024-11-14T11:56:35.264513Z","shell.execute_reply.started":"2024-11-14T11:56:35.253995Z","shell.execute_reply":"2024-11-14T11:56:35.263536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFBertModel\n\n# Define maximum sequence length\nmax_seq_length = 100  # Adjust according to your model/data\n\n# Define the number of classes for your classification task\nnum_classes = 18  # Adjust based on your dataset\n\ndef create_model():\n    # Load BERT model from Hugging Face\n    bert_model = TFBertModel.from_pretrained('aubmindlab/bert-base-arabertv02')\n\n    # Input layers for BERT\n    input_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"attention_mask\")\n    token_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"token_type_ids\")\n    \n    # BERT Model Outputs\n    bert_outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n    pooled_output = bert_outputs.pooler_output  # [CLS] token output for classification tasks\n\n    # POS and NER Inputs\n    pos_input = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"pos_input\")\n    ner_input = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"ner_input\")\n\n    # Embedding layers for POS and NER inputs\n    pos_embedding = tf.keras.layers.Embedding(input_dim=50, output_dim=16, input_length=max_seq_length)(pos_input)\n    ner_embedding = tf.keras.layers.Embedding(input_dim=50, output_dim=16, input_length=max_seq_length)(ner_input)\n\n    # Flatten the embeddings\n    pos_flat = tf.keras.layers.Flatten()(pos_embedding)\n    ner_flat = tf.keras.layers.Flatten()(ner_embedding)\n\n    # Concatenate BERT output with POS and NER embeddings\n    combined_features = tf.keras.layers.Concatenate(axis=-1)([pooled_output, pos_flat, ner_flat])\n\n    # Fully connected hidden layers\n    hidden_layer1 = tf.keras.layers.Dense(128, activation='relu')(combined_features)\n    hidden_layer2 = tf.keras.layers.Dense(64, activation='relu')(hidden_layer1)\n    hidden_layer3 = tf.keras.layers.Dense(32, activation='relu')(hidden_layer2)\n\n    # Output layer with sigmoid activation for multi-label classification\n    output = tf.keras.layers.Dense(num_classes, activation='sigmoid')(hidden_layer3)\n\n    # Define the model\n    model = tf.keras.Model(\n        inputs=[input_ids, attention_mask, token_type_ids, pos_input, ner_input],\n        outputs=output\n    )\n\n    return model\n\n# Create the model\nmodel = create_model()\n\n# Compile the model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Display the model summary to verify the architecture\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:56:35.265926Z","iopub.execute_input":"2024-11-14T11:56:35.267238Z","iopub.status.idle":"2024-11-14T11:57:06.663141Z","shell.execute_reply.started":"2024-11-14T11:56:35.267187Z","shell.execute_reply":"2024-11-14T11:57:06.661975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n# Define the weighted loss function\n#loss = tf.keras.losses.BinaryCrossentropy(from_logits=False, pos_weight=18)\nloss= 'binary_crossentropy'\n#loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\naccuracy=tf.keras.metrics.CategoricalAccuracy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:57:06.664883Z","iopub.execute_input":"2024-11-14T11:57:06.665272Z","iopub.status.idle":"2024-11-14T11:57:06.676208Z","shell.execute_reply.started":"2024-11-14T11:57:06.665232Z","shell.execute_reply":"2024-11-14T11:57:06.675095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compile_and_fit(model_name, optimizer, loss_fn, accuracy, name, epochs_num, batch_size, steps_per_epoch):\n\n\n\n\n    model_name.compile(optimizer=optimizer,\n                loss=loss_fn,\n                metrics=[\n                  tf.keras.metrics.BinaryCrossentropy(\n                      from_logits=False, name='binary_crossentropy'),\n                  'accuracy'])\n\n    model_name.summary()\n\n    history = model.fit({\n        'input_ids': train_inputs[0],\n        'attention_mask': train_inputs[1],\n        'token_type_ids': train_inputs[2],\n        'pos_input': pos_train_input,\n        'ner_input': ner_train_input\n    },y_train,\n                  validation_data=({\n            'input_ids': val_inputs[0],\n            'attention_mask': val_inputs[1],\n            'token_type_ids': val_inputs[2],\n            'pos_input': pos_val_input,\n            'ner_input': ner_val_input\n        },y_val),\n                        epochs=epochs_num,batch_size=batch_size,steps_per_epoch = steps_per_epoch,\n                        callbacks=get_callbacks(name),\n                        verbose=1)\n\n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:57:06.677728Z","iopub.execute_input":"2024-11-14T11:57:06.678216Z","iopub.status.idle":"2024-11-14T11:57:06.688564Z","shell.execute_reply.started":"2024-11-14T11:57:06.678154Z","shell.execute_reply":"2024-11-14T11:57:06.687544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"size_histories = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:57:06.689826Z","iopub.execute_input":"2024-11-14T11:57:06.690194Z","iopub.status.idle":"2024-11-14T11:57:06.704379Z","shell.execute_reply.started":"2024-11-14T11:57:06.690155Z","shell.execute_reply":"2024-11-14T11:57:06.703456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install git+https://github.com/tensorflow/docs\nimport tensorflow_docs as tfdocs\nimport tensorflow_docs.modeling\nimport tensorflow_docs.plots","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:57:06.705696Z","iopub.execute_input":"2024-11-14T11:57:06.706170Z","iopub.status.idle":"2024-11-14T11:57:27.594157Z","shell.execute_reply.started":"2024-11-14T11:57:06.706117Z","shell.execute_reply":"2024-11-14T11:57:27.593016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"size_histories['ORIGINAL'] = compile_and_fit(model, optimizer, loss, accuracy, 'ORIGINAL', EPOCHS, BATCH_SIZE, STEPS_PER_EPOCH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T11:57:27.595825Z","iopub.execute_input":"2024-11-14T11:57:27.596271Z","iopub.status.idle":"2024-11-14T12:29:34.553482Z","shell.execute_reply.started":"2024-11-14T11:57:27.596229Z","shell.execute_reply":"2024-11-14T12:29:34.552386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plotter = tfdocs.plots.HistoryPlotter(metric = 'binary_crossentropy', smoothing_std=10)\nplotter.plot(size_histories)\n\n#plt.ylim([-1, 2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T12:29:34.555011Z","iopub.execute_input":"2024-11-14T12:29:34.555492Z","iopub.status.idle":"2024-11-14T12:30:10.259596Z","shell.execute_reply.started":"2024-11-14T12:29:34.555432Z","shell.execute_reply":"2024-11-14T12:30:10.258458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure you're passing the correct input format for predictions (similar to training)\ntest_scores = model.predict({\n    'input_ids': test_inputs[0],\n    'attention_mask': test_inputs[1],\n    'token_type_ids': test_inputs[2],\n    'pos_input': pos_test_input,\n    'ner_input': ner_test_input\n})\n\n# Convert scores to binary labels (0 or 1) based on a threshold of 0.5\ntest_pred_labels = (test_scores >= 0.5).astype(int)\n\n# True labels\ntest_true_labels = y_ts.astype(int)\n\n# Optionally, evaluate accuracy or other metrics\nfrom sklearn.metrics import accuracy_score, f1_score, hamming_loss\n\n# Calculate accuracy, F1 score, and Hamming loss\naccuracy = accuracy_score(test_true_labels, test_pred_labels)\nf1 = f1_score(test_true_labels, test_pred_labels, average='micro')\nhamming = hamming_loss(test_true_labels, test_pred_labels)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"F1 Score (micro): {f1}\")\nprint(f\"Hamming Loss: {hamming}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T12:30:10.261002Z","iopub.execute_input":"2024-11-14T12:30:10.261713Z","iopub.status.idle":"2024-11-14T12:30:23.791030Z","shell.execute_reply.started":"2024-11-14T12:30:10.261655Z","shell.execute_reply":"2024-11-14T12:30:23.789966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, recall_score,classification_report\n\n# Calculate F1 score\nf1_macro = f1_score(test_true_labels, test_pred_labels, average='macro')\nf1_micro = f1_score(test_true_labels, test_pred_labels, average='micro')\n\n# Calculate confusion matrix, accuracy, and recall\nconfusion_mat = confusion_matrix(test_true_labels.flatten(), test_pred_labels.flatten())\naccuracy = accuracy_score(test_true_labels.flatten(), test_pred_labels.flatten())\nrecall = recall_score(test_true_labels.flatten(), test_pred_labels.flatten(), average='macro')\nclassification_rep = classification_report(test_true_labels.flatten(), test_pred_labels.flatten())\n\nfrom sklearn import metrics\n\nprint(\"Classification Report:\")\nprint(classification_rep)\nprint(\"F1 macro score:\", f1_macro)\nprint(\"F1 micro score:\", f1_micro)\nprint(\"Confusion Matrix:\")\nprint(confusion_mat)\nprint(\"Accuracy:\", accuracy)\nprint(\"Recall:\", recall)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T12:30:23.799945Z","iopub.execute_input":"2024-11-14T12:30:23.800355Z","iopub.status.idle":"2024-11-14T12:30:23.831429Z","shell.execute_reply.started":"2024-11-14T12:30:23.800315Z","shell.execute_reply":"2024-11-14T12:30:23.830511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Function to map binary labels to actual label names\ndef binary_to_text_labels(binary_labels, class_names):\n    return [class_names[i] for i in range(len(binary_labels)) if binary_labels[i] == 1]\n\n# Create a list to store the results\nresults = []\n\n# Iterate over the test samples and collect the text, actual labels, and predicted labels\nfor i in range(len(x_ts)):  # test_texts is your list of test samples\n    actual_label_names = binary_to_text_labels(test_true_labels[i], classes)  # using 'classes' array\n    predicted_label_names = binary_to_text_labels(test_pred_labels[i], classes)  # using 'classes' array\n\n    # Append the results to the list\n    results.append({\n        \"Text\": x_ts[i],\n        \"Actual Labels\": ', '.join(actual_label_names),\n        \"Predicted Labels\": ', '.join(predicted_label_names)\n    })\n\n# Convert the results list to a pandas DataFrame\nresults_df = pd.DataFrame(results)\n\n# Display the DataFrame\nresults_df  # Use .head() to display the first few rows\n\n# If you want to display the entire table, you can use:\n# pd.set_option('display.max_rows', None)  # Uncomment this line to display all rows if needed\n# display(results_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T12:30:23.832677Z","iopub.execute_input":"2024-11-14T12:30:23.833330Z","iopub.status.idle":"2024-11-14T12:30:23.862766Z","shell.execute_reply.started":"2024-11-14T12:30:23.833283Z","shell.execute_reply":"2024-11-14T12:30:23.861861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the model to a file\nmodel.save(\"/kaggle/working/predict_resuls/BERT_1/augOriginal/cnn_lstm_attention_model.h5\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T12:30:23.864145Z","iopub.execute_input":"2024-11-14T12:30:23.864834Z","iopub.status.idle":"2024-11-14T12:30:26.702641Z","shell.execute_reply.started":"2024-11-14T12:30:23.864787Z","shell.execute_reply":"2024-11-14T12:30:26.701510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the results to a CSV file\nresults_df.to_csv(\"/kaggle/working/predict_resuls/BERT_1/augOriginal/results_df.csv\", index=False)\n\n# Alternatively, save to an Excel file\nresults_df.to_excel(\"/kaggle/working/predict_resuls/BERT_1/augOriginal/results_df.xlsx\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T12:30:26.704458Z","iopub.execute_input":"2024-11-14T12:30:26.704879Z","iopub.status.idle":"2024-11-14T12:30:26.989219Z","shell.execute_reply.started":"2024-11-14T12:30:26.704831Z","shell.execute_reply":"2024-11-14T12:30:26.988095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save evaluation metrics to a text file\nwith open(\"/kaggle/working/predict_resuls/BERT_1/augOriginal/evaluation_metrics.txt\", \"w\") as file:\n    file.write(\"Classification Report:\\n\")\n    file.write(classification_rep)\n    file.write(f\"\\nF1 macro score: {f1_macro}\\n\")\n    file.write(f\"F1 micro score: {f1_micro}\\n\")\n    file.write(f\"\\nConfusion Matrix:\\n{confusion_mat}\\n\")\n    file.write(f\"Accuracy: {accuracy}\\n\")\n    file.write(f\"Recall: {recall}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T12:30:26.990585Z","iopub.execute_input":"2024-11-14T12:30:26.991340Z","iopub.status.idle":"2024-11-14T12:30:26.998480Z","shell.execute_reply.started":"2024-11-14T12:30:26.991302Z","shell.execute_reply":"2024-11-14T12:30:26.997412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_loss(size_histories):\n    plt.figure(figsize=(10, 6))\n\n    # Assuming 'ORIGINAL' is the key you want to plot\n    key = 'ORIGINAL'\n\n    history = size_histories[key]\n    plt.plot(history.history['val_loss'], label=f'{key} Validation Loss')\n    plt.plot(history.history['loss'], label=f'{key} Training Loss')\n\n    plt.title(\"Training and Validation Loss over Epochs\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_loss(size_histories)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T12:30:26.999719Z","iopub.execute_input":"2024-11-14T12:30:27.000028Z","iopub.status.idle":"2024-11-14T12:30:27.367869Z","shell.execute_reply.started":"2024-11-14T12:30:26.999994Z","shell.execute_reply":"2024-11-14T12:30:27.366375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_and_save_size_histories(size_histories, filename):\n    plt.figure(figsize=(10, 6))\n\n    for key, history in size_histories.items():\n        plt.plot(history.history['val_loss'], label=f'{key} Validation Loss')\n        plt.plot(history.history['loss'], '--', label=f'{key} Training Loss')\n\n    plt.title(\"Training and Validation Loss over Epochs\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n\n    # Save the plot to a file\n    plt.savefig(filename)\n    plt.close()  # Close the plot to prevent displaying it if running in a loop\n\n# Save the plot\nplot_and_save_size_histories(size_histories, filename=\"/kaggle/working/predict_resuls/BERT_1/augOriginal/Training and Validation Loss over Epochs.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T12:30:27.369160Z","iopub.execute_input":"2024-11-14T12:30:27.369646Z","iopub.status.idle":"2024-11-14T12:30:27.672265Z","shell.execute_reply.started":"2024-11-14T12:30:27.369605Z","shell.execute_reply":"2024-11-14T12:30:27.671399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training and validation accuracy\ndef plot_accuracy(size_histories):\n    plt.figure(figsize=(10, 6))\n\n    for key, history in size_histories.items():\n        plt.plot(history.history['val_accuracy'], label=f'{key} Validation Accuracy')\n        plt.plot(history.history['accuracy'], '--', label=f'{key} Training Accuracy')\n\n    plt.title(\"Training and Validation Accuracy over Epochs\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_accuracy(size_histories)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T12:30:27.673507Z","iopub.execute_input":"2024-11-14T12:30:27.673874Z","iopub.status.idle":"2024-11-14T12:30:28.027283Z","shell.execute_reply.started":"2024-11-14T12:30:27.673836Z","shell.execute_reply":"2024-11-14T12:30:28.026320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_and_save_size_histories(size_histories, filename):\n    plt.figure(figsize=(10, 6))\n\n    for key, history in size_histories.items():\n        plt.plot(history.history['val_accuracy'], label=f'{key} Validation Accuracy')\n        plt.plot(history.history['accuracy'], '--', label=f'{key} Training Accuracy')\n\n    plt.title(\"Training and Validation Accuracy over Epochs\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.grid(True)\n\n\n    # Save the plot to a file\n    plt.savefig(filename)\n    plt.close()  # Close the plot to prevent displaying it if running in a loop\n\n# Save the plot\nplot_and_save_size_histories(size_histories, filename=\"/kaggle/working/predict_resuls/BERT_1/augOriginal/Training and Validation Accuracy over Epochs.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T12:30:28.028790Z","iopub.execute_input":"2024-11-14T12:30:28.029322Z","iopub.status.idle":"2024-11-14T12:30:28.314998Z","shell.execute_reply.started":"2024-11-14T12:30:28.029273Z","shell.execute_reply":"2024-11-14T12:30:28.313896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training and validation binary crossentropy\ndef plot_binary_crossentropy(size_histories):\n    plt.figure(figsize=(10, 6))\n\n    for key, history in size_histories.items():\n        plt.plot(history.history['val_binary_crossentropy'], label=f'{key} Validation Binary Crossentropy')\n        plt.plot(history.history['binary_crossentropy'], '--', label=f'{key} Training Binary Crossentropy')\n\n    plt.title(\"Training and Validation Binary Crossentropy over Epochs\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Binary Crossentropy\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_binary_crossentropy(size_histories)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T12:30:28.316292Z","iopub.execute_input":"2024-11-14T12:30:28.316653Z","iopub.status.idle":"2024-11-14T12:30:28.698251Z","shell.execute_reply.started":"2024-11-14T12:30:28.316609Z","shell.execute_reply":"2024-11-14T12:30:28.697278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_and_save_size_histories(size_histories, filename):\n    plt.figure(figsize=(10, 6))\n\n    for key, history in size_histories.items():\n        plt.plot(history.history['val_binary_crossentropy'], label=f'{key} Validation Binary Crossentropy')\n        plt.plot(history.history['binary_crossentropy'], '--', label=f'{key} Training Binary Crossentropy')\n\n    plt.title(\"Training and Validation Binary Crossentropy over Epochs\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Binary Crossentropy\")\n    plt.legend()\n    plt.grid(True)\n\n    # Save the plot to a file\n    plt.savefig(filename)\n    plt.close()  # Close the plot to prevent displaying it if running in a loop\n\n# Save the plot\nplot_and_save_size_histories(size_histories, filename=\"/kaggle/working/predict_resuls/BERT_1/augOriginal/Training and Validation Binary Crossentropy over Epochs.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T12:30:28.699724Z","iopub.execute_input":"2024-11-14T12:30:28.700188Z","iopub.status.idle":"2024-11-14T12:30:29.016820Z","shell.execute_reply.started":"2024-11-14T12:30:28.700141Z","shell.execute_reply":"2024-11-14T12:30:29.015898Z"}},"outputs":[],"execution_count":null}]}